\documentclass[a4paper,twoside,11pt]{article}
\usepackage{a4wide,graphicx,fancyhdr,amsmath,amssymb}
\usepackage{a4wide,graphicx,fancyhdr,amssymb,float,hyperref}
\usepackage{multirow,array,tabularx,afterpage}
\usepackage{amsmath,enumitem}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}

%----------------------- Macros and Definitions --------------------------

\setlength\headheight{20pt}
\addtolength\topmargin{-10pt}
\addtolength\footskip{20pt}

\fancypagestyle{plain}{%
\fancyhf{}
\fancyhead[LO,RE]{\sffamily\bfseries\large Group assignment 2}
\fancyhead[RO,LE]{\sffamily\bfseries\large 2ID90 Artificial Intelligence}
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[RO,LE]{\sffamily\bfseries\large Group assignment 2}
\fancyhead[LO,RE]{\sffamily\bfseries\large 2ID90 Artificial Intelligence}
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{0pt}

%-------------------------------- Title ----------------------------------

\title{\vspace{-\baselineskip}\sffamily\bfseries Spellchecker}
\author{\begin{tabular}{rl}
  Martijn Gösgens & \qquad  0914954 \\
  Dominique Sommers & \qquad 0895679 \\ \end{tabular}}

\date{\today}

%--------------------------------- Text ----------------------------------

\begin{document}
\maketitle
\section{Algorithms}
\subsection{Candidate words}
\begin{algorithm}[H]
\caption{getCandidateWords(String word, double numWords)}
\begin{algorithmic}
\State ZET IK NOG OM IN PSEUDOCODE...
\State $mapOfWords$ to be filled with all candidate words
\State Get all words by inserting one letter for every letter in the alphabet at one position of the word for every possible position of the word and put them in $mapOfWords$.
\State Get all words by deleting one letter of the word for every letter of the word and put them in $mapOfWords$.
\State Get all words by transpositioning two letters of the word for every two letters of the word and put them in $mapOfWords$.
\State Get all words by substituting a letter of the word with another letter for every letter in the alphabet and every letter of the word and put them in $mapOfWords$.
\State For all words in $mapOfWords$, the noisy channel probability is added by dividing the confusion count with the normalization of the word.
\State $resultMapOfWords$ to be filled with possible good candidate words
\State Put all candidate words with a higher probability than 0 in $resultMapOfWords$.
\State For all words in $resultMapOfWords$, the word probability is added by taking the sum of the $log_10$ of the noisy channel probability and the $log_10$ of the word probability as the frequency of the word divided by the number of monograms + the number of words.
\State Put the word itself if it is contained in the vocabulary and add the word probability of the $log_10$ of the number of words - 1 divided by the number of words if the number of words is higher than 1. Else add word probability of 0.5.
\State Return $resultMapOfWords$
\end{algorithmic}
\end{algorithm}

This function gets all the candidate words of the input word. These candidate words have a maximum Damerau-Levenshtein distance of 1 which means that the word is only altered by at most 1 insertion, deletion, transposition or substitution. The candidate words with a noisy channel probability of 0 will then be deleted and the word probability is calculated. This is determined by the sum of the noisy channel probability and the $log_10$ of the frequency of the word divided by the number of monograms added to the number of words.
Then the word itself is added if it is contained in the vocabular and the word probability is determined by the number of words - 1 divided by the number of words if the number of words is higher than 1. Otherwise, the word probability will be 0.5.

\subsection{Find correct word}
\begin{algorithm}[H]
\caption{findCorrect(String previousSentence, String nextSentence, double currentProbability, int correctionsMade)}
\begin{algorithmic}
\State TODO
\end{algorithmic}
\end{algorithm}

\subsection{Good-Turing Smoothing}
\begin{algorithm} [H]
\caption{getSmoothedCount(String $NGram$)}\label{euclid}
\begin{algorithmic}
\State Construct a sorted map $freqOfFreq$ with the frequency of all occurring words as keys and the frequency of these frequencies as values
\State $maxFreq \gets$ the most frequent word
\State $sumCounts \gets$ the sum of all frequencies
\State $c \gets$ the frequency of the word $NGram$
\If {$c == 0$}
  \State Return $freqOfFreq.get(1) / sumCounts$
\ElsIf {$c == maxFreq$}
  \State Return $c / sumCounts$
\Else
  \State $N_c \gets freqOfFreq.get(c)$
  \State $nextC \gets$ the lowest frequency higher than $c$
  \State $N_{c+x} \gets freqOfFreq.get(nextC)$
  \State Return $(c + x) * N_{c+x} / N_c$
\EndIf
\end{algorithmic}
\end{algorithm}

For the smoothing of the counts of the words, we chose to use Good-Turing Smoothing with a slight adjustment. The smoothing of the counts is to determine the chance of a word occurring again. The smoothed count will be slightly lower than the concrete count since there are words that have never occurred before. This difference is determined by the frequency of the frequency of the words. With the Good-Turing algorithm the smoothed count will be determined be the frequency of the frequency of the word and the freq. of the frequency + 1. But since there are gaps between frequencies, especially when the frequency gets higher, it serves no value to take the freq. of the frequency + 1 since this is zero. In our implementation we therefore take the next occurring frequency to get a value. When the gap between frequencies is high, this gets more unreliable. There is a solution to avoid this by drawing a line of the frequencies which is the best-fit power law, but we did not have the time and knowledge to implement this yet.

\section{Results}
The results of our spell checker are quite good. Most of the words are corrected correctly, with some exceptions. There are however also some words that are spelled correctly, but are corrected. This problem appears mostly when the correction is a word which occurs much more frequent than the corrected word, but in the particular sentence, the corrected word is in fact the correct word.

\section{References}
\begin{enumerate}
\item None
\end{enumerate}

\section{Contribution}
Martijn Gösgens: TODO. \\
Dominique Sommers: I implemented the Good-Turing Smoothing algorithm and decided to take the next frequency instead of add-one smoothening or the more complicated Good-Turing Smoothing with the best-fit power law. Besides that, I wrote most part of the report. I also made a function to get the best candidate word for every word in the sentence, but this was later adjusted and corrected by Martijn adding the bigrams of the sentences.

\section{Manual}
The spellchecker takes sentences as input. These can consist of multiple words and multiple sentences. An important thing to note is that the spellchecker can only correct words with a Damerau-Levenshtein distance of 1 which means that each input word may only be altered by at most 1 insertion, deletion, transposition or substitution. The words in the input sentences may not have interpunction or any other kind of character than the english alphabet except for the single apostrophe ('). All letters of all words should be in lower case. After giving the input, the spellchecker will check for spelling mistakes and automatically replace them to the correct spelling of the word.

\end{document}
