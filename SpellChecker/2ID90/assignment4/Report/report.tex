\documentclass[a4paper,twoside,11pt]{article}
\usepackage{a4wide,graphicx,fancyhdr,amsmath,amssymb}
\usepackage{a4wide,graphicx,fancyhdr,amssymb,float,hyperref}
\usepackage{multirow,array,tabularx,afterpage}
\usepackage{amsmath,enumitem}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}

%----------------------- Macros and Definitions --------------------------

\setlength\headheight{20pt}
\addtolength\topmargin{-10pt}
\addtolength\footskip{20pt}

\fancypagestyle{plain}{%
\fancyhf{}
\fancyhead[LO,RE]{\sffamily\bfseries\large Group assignment 2}
\fancyhead[RO,LE]{\sffamily\bfseries\large 2ID90 Artificial Intelligence}
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[RO,LE]{\sffamily\bfseries\large Group assignment 2}
\fancyhead[LO,RE]{\sffamily\bfseries\large 2ID90 Artificial Intelligence}
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{0pt}

%-------------------------------- Title ----------------------------------

\title{\vspace{-\baselineskip}\sffamily\bfseries Spellchecker}
\author{\begin{tabular}{rl}
  Martijn Gösgens & \qquad  0914954 \\
  Dominique Sommers & \qquad 0895679 \\ \end{tabular}}

\date{\today}

%--------------------------------- Text ----------------------------------

\begin{document}
\maketitle
\section{Algorithms}
\subsection{Alpha-Beta Search}
\begin{algorithm}[H]
\caption{Alpha-Beta-Search(int depth)}
\begin{algorithmic}
	\State Let s be the current state
	\State bestMove $\gets$ null
	\State bestValue $\gets -\infty$
	\ForAll{Moves m}
		\State Do(m)
		\State value $\gets$ AlphaBetaMin(s, $-\infty$, $\infty$, depth)
		\If{value $>$ bestValue}
			\State bestValue $\gets$ value
			\State bestMove $\gets m$
		\EndIf
		\State Undo(m)
	\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Alpha-Beta-Min(State s, double alpha, double beta, int d)}
\begin{algorithmic}
	\State Check whether the time is up and throw an exception if so
	\If{d $=$ 0}
		\Return State-Evaluation(s)
	\Else
		\ForAll{Moves m}
			\State Do(m)
			\State max $\gets$ Alpha-Beta-Max(s, alpha, beta, d-1)
			\State beta $\gets$ Min(beta, max)
			\State Undo(m)
			\If{alpha $\geq$ beta}
				\Return alpha
			\EndIf
		\EndFor
		\Return beta
	\EndIf
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Alpha-Beta-Max(State s, double alpha, double beta, int d)}
\begin{algorithmic}
	\State Check whether the time is up and throw an exception if so
	\If{d $=$ 0}
		\Return State-Evaluation(s)
	\Else
		\ForAll{Moves m}
			\State Do(m)
			\State max $\gets$ Alpha-Beta-Min(s, alpha, beta, d-1)
			\State alpha $\gets$ Max(beta, max)
			\State Undo(m)
			\If{alpha $\geq$ beta}
				\Return beta
			\EndIf
		\EndFor
		\Return alpha
	\EndIf
\end{algorithmic}
\end{algorithm}

We implemented the Alpha-Beta algorithm as described in "Introduction to Articial Intelligence" [2]. We decided not to use GameNode's since we did not think that would be necessary and this might be more efficient.

\subsection{Iterative Deepening}
\begin{algorithm}[H]
\caption{Iterative-Deepening()}
\begin{algorithmic}
	\State bestMove $\gets$ null
	\For{$d=1, 2, ...$}
		\State try bestMove $\gets$ Alpha-Beta-Search(d) and return bestMove if exception thrown
	\EndFor
\end{algorithmic}
\end{algorithm}

We call our Alpha-Beta-Search algorithm with increasing depths and when the AI is stopped (an exception is thrown), we return the move found by the last finished run of the algorithm.

\subsection{Evaluation Function}
\begin{algorithm} [H]
\caption{State-Evaluation[State s]}\label{euclid}
\begin{algorithmic}
\If {AI has no moves left}
  \State Return $Integer.MINVALUE$
\ElsIf {Opponent has no moves left}
  \State Return $Integer.MAXVALUE$
\Else
  \ForAll {$pieces$  $p$}
    \State $pieceValue = 100$
    \If {$p$ is a king}
      \State $p.pieceValue *= 2$
      \If {$p$ is on the side}
        \State $p.pieceValue += 20$
      \EndIf
    \ElsIf {$p$ is black}
      \If {$p$ is on row $9$}
        \State $p.pieceValue += 20$
      \Else
        \State $p.pieceValue += (9-rowNumber) * 3$
      \EndIf
    \Else
      \If {$p$ is on row $0$}
        \State $p.pieceValue += 20$
      \Else
        \State $p.pieceValue += rowNumber * 3$ 
      \EndIf
    \EndIf
  \EndFor
  
  \State blackValue $\sum_{p\epsilon pieces} p.pieceValue $ where $p$ is black.
  \State whiteValue $\sum_{p\epsilon pieces} p.pieceValue $ where $p$ is white.
  \If {AI is playing black}
    \State return $blackValue / whiteValue$
  \Else
    \State return $whiteValue / blackValue$ 
  \EndIf
\EndIf
\end{algorithmic}
\end{algorithm}

For the evaluation function, a score system is made for the draughts pieces. Initially, every piece is worth 100 points. A king is worth twice as much as a normal piece. Now, for kings it is better to stay at the side of the board rather than somewhere in the middle [1]. So there are 20 points added to the score of a king when it is positioned at the side in order to give this state of position a higher score.
For normal pieces, the defense line is valuable to keep the opponent from getting a king. So again, 20 points are added to the score of pieces positioned on the first line. Pieces further on the board are more likely to get jumped, but is also more likely to jump it self. Also, the closer it is to the opponent's side, the closer it is to become a king. For every line it is moved forward, the piece receives an extra 3 points to add to it's score. So, a maximum of 24 points can be added, when the piece is one row from becoming a king.\\
After evaluating each piece value, the final value of the state is calculated as the ratio between the value of the AI's pieces and the value of the opponent's pieces. This will give a better approximation to the value of the state than taking the difference between the two, since for example the state with 5 pieces to 1 is much strong than with 20 pieces to 16, although the difference is the same. 

\section{Results}
Depending on the allowed time and the state of the board, the Alpha-Beta search usually reaches depths between 6 and 10. This can be seen by letting the player play a match and watching the application output (the depths are printed). The player itself also gives good results; it wins from all the default players and it also won from every human being that we let it play to (we tested it on seven people). This result can be reproduced by trying to play against it, you will very likely lose. Our evaluation function also gave good results. If we copy the AlphaBetaPlayer and replace its evaluationfunction with the more obvious evaluation function of counting every piece as one, counting kings as two and returning the difference of scores between the team, then this player will lose against our AlphaBetaPlayer.

\section{References}
\begin{enumerate}
\item http://www.checkerstip.com/handling-a-king-piece-in.html
\item Ertel, W. Introduction to Artificial Intelligence.
\end{enumerate}

\section{Contribution}
Martijn Gösgens: I implemented and tested the Alpha-Beta search with iterative deepening. I also tested the evaluation function and contributed to it slightly (I came up with the idea of looking at the relative difference of the scores instead of the absolute difference). \\
Dominique Sommers: I designed the evaluation function for the AI. In order to do this, I studied good draughts patterns and searched for strong positions of particular pieces. These could be defended pieces (having a piece behind them, so they can not be jumped), defending pieces (having a piece in front of them), elbow pattern (pieces defending and being defended). However, I decided to keep the evaluation simple, since the method is called many times.

\section{Manual}
Add the AI player name to the Draughts game and it can play against other AI's or against a human. You can let it play against the default players and see it win every time, you can let a few average people play against it and see the majority lose. You can also let it play against the older versions of the AlphaBetaPlayer (AlphaBetaPlayerV0, with evaluation function as described in the results) and see it win.

\end{document}
