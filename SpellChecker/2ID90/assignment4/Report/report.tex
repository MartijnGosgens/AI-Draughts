\documentclass[a4paper,twoside,11pt]{article}
\usepackage{a4wide,graphicx,fancyhdr,amsmath,amssymb}
\usepackage{a4wide,graphicx,fancyhdr,amssymb,float,hyperref}
\usepackage{multirow,array,tabularx,afterpage}
\usepackage{amsmath,enumitem}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}

%----------------------- Macros and Definitions --------------------------

\setlength\headheight{20pt}
\addtolength\topmargin{-10pt}
\addtolength\footskip{20pt}

\fancypagestyle{plain}{%
\fancyhf{}
\fancyhead[LO,RE]{\sffamily\bfseries\large Group assignment 2}
\fancyhead[RO,LE]{\sffamily\bfseries\large 2ID90 Artificial Intelligence}
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[RO,LE]{\sffamily\bfseries\large Group assignment 2}
\fancyhead[LO,RE]{\sffamily\bfseries\large 2ID90 Artificial Intelligence}
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{0pt}

%-------------------------------- Title ----------------------------------

\title{\vspace{-\baselineskip}\sffamily\bfseries Spellchecker}
\author{\begin{tabular}{rl}
  Martijn Gösgens & \qquad  0914954 \\
  Dominique Sommers & \qquad 0895679 \\ \end{tabular}}

\date{\today}

%--------------------------------- Text ----------------------------------

\begin{document}
\maketitle
\section{Algorithms}
\subsection{Candidate words}
\begin{algorithm}[H]
\caption{getCandidateWords(String word, double numWords)}
\begin{algorithmic}
\State $mapOfWords$ to be filled with all candidate words
\ForAll {Letters in the alphabet}
  \ForAll {Positions in word}
    \State Insert letter in position
    \State Put resulting string in $mapOfWords$
  \EndFor
\EndFor
\ForAll {Letters in word}
  \State Delete letter from word
  \State Put resulting string in $mapOfWords$
\EndFor
\ForAll {Letters "a" in word}
  \ForAll {Letters "b" in word}
    \State Swap letters "a" and "b" in word
    \State Put resulting string in $mapOfWords$
  \EndFor
\EndFor
\ForAll {Letters "a" in the alphabet}
  \ForAll {Letters "b" in word}
    \State Replace letter "b" with letter "a" in word
    \State Put resulting string in $mapOfWords$
  \EndFor
\EndFor
\State $resultMapOfWords$ to be filled with possible good candidate words
\ForAll {Words in $mapOfWords$}
  \State $Noisy Channel Probability \gets confusion Count / Normalization$
  \If {$Noisy Channel Probability > 0$}
	\State Put the candidate word in $resultMapOfWords$
	\State $Probability \gets log_{10}(Noisy Channel Probability) + log_{10}(Word Probability)$
  \EndIf
\EndFor
\If {word itself is contained in the vocabulary}
  \State Put word in $resultMapOfWords$.
  \If {$\#words > 1$}
  	\State $Probability \gets log_{10}((\#words - 1) / \#words)$
  \Else
    \State $Probability \gets log_{10}(0.5)$
  \EndIf
\EndIf    
\State Return $resultMapOfWords$
\end{algorithmic}
\end{algorithm}

This function gets all the candidate words of the input word. These candidate words have a maximum Damerau-Levenshtein distance of 1 which means that the word is only altered by at most 1 insertion, deletion, transposition or substitution. The candidate words with a noisy channel probability of 0 will then be deleted and the word probability is calculated. This is determined by the sum of the noisy channel probability and the $log_10$ of the frequency of the word divided by the number of monograms added to the number of words.
Then the word itself is added if it is contained in the vocabular and the word probability is determined by the number of words - 1 divided by the number of words if the number of words is higher than 1. Otherwise, the word probability will be 0.5.

\subsection{Find correct word}
\begin{algorithm}[H]
\caption{findCorrect(String previousSentence, String nextSentence, double currentProbability, int correctionsMade)}
\begin{algorithmic}
\State TODO
\end{algorithmic}
\end{algorithm}

\subsection{Good-Turing Smoothing}
\begin{algorithm} [H]
\caption{getSmoothedCount(String $NGram$)}\label{euclid}
\begin{algorithmic}
\State Construct a sorted map $freqOfFreq$ with the frequency of all occurring words as keys and the frequency of these frequencies as values
\State $maxFreq \gets$ the most frequent word
\State $sumCounts \gets$ the sum of all frequencies
\State $c \gets$ the frequency of the word $NGram$
\If {$c == 0$}
  \State Return $freqOfFreq.get(1) / sumCounts$
\ElsIf {$c == maxFreq$}
  \State Return $c / sumCounts$
\Else
  \State $N_c \gets freqOfFreq.get(c)$
  \State $nextC \gets$ the lowest frequency higher than $c$
  \State $N_{c+x} \gets freqOfFreq.get(nextC)$
  \State Return $(c + x) * N_{c+x} / N_c$
\EndIf
\end{algorithmic}
\end{algorithm}

For the smoothing of the counts of the words, we chose to use Good-Turing Smoothing with a slight adjustment. The smoothing of the counts is to determine the chance of a word occurring again. The smoothed count will be slightly lower than the concrete count since there are words that have never occurred before. This difference is determined by the frequency of the frequency of the words. With the Good-Turing algorithm the smoothed count will be determined be the frequency of the frequency of the word and the freq. of the frequency + 1. But since there are gaps between frequencies, especially when the frequency gets higher, it serves no value to take the freq. of the frequency + 1 since this is zero. In our implementation we therefore take the next occurring frequency to get a value. When the gap between frequencies is high, this gets more unreliable. There is a solution to avoid this by drawing a line of the frequencies which is the best-fit power law, but we did not have the time and knowledge to implement this yet.

\section{Results}
The results of our spell checker are quite good. Most of the words are corrected correctly, with some exceptions. There are however also some words that are spelled correctly, but are corrected. This problem appears mostly when the correction is a word which occurs much more frequent than the corrected word, but in the particular sentence, the corrected word is in fact the correct word.

\section{References}
\begin{enumerate}
\item None
\end{enumerate}

\section{Contribution}
Martijn Gösgens: TODO. \\
Dominique Sommers: I implemented the Good-Turing Smoothing algorithm and decided to take the next frequency instead of add-one smoothening or the more complicated Good-Turing Smoothing with the best-fit power law. Besides that, I wrote most part of the report. I also made a function to get the best candidate word for every word in the sentence, but this was later adjusted and corrected by Martijn adding the bigrams of the sentences.

\section{Manual}
The spellchecker takes sentences as input. These can consist of multiple words and multiple sentences. An important thing to note is that the spellchecker can only correct words with a Damerau-Levenshtein distance of 1 which means that each input word may only be altered by at most 1 insertion, deletion, transposition or substitution. The words in the input sentences may not have interpunction or any other kind of character than the english alphabet except for the single apostrophe ('). All letters of all words should be in lower case. After giving the input, the spellchecker will check for spelling mistakes and automatically replace them to the correct spelling of the word.

\end{document}
